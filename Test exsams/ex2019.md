# Problem 1

Answer the following questions.

## Question 1.1

Why is it important that the source register addresses are always at the same position in the instruction encoding of the RISC-V instruction set?

Answer:

    All instruction is the same size.

    Keeping the instruction formats as similar as possible reduces hardware complexity.

    Similarly, the opcode and funct3 fields are the same size in all locations, and they are always in the same place.


## Question 1.2

Name the two types of locality and give one example for each when it happens.

Answer:

**temporal locality** The locality principle stating that if a data location is referenced then it will tend to be referenced again soon.

**spatial locality** The locality principle stating that if a data location is referenced, data locations with nearby addresses will tend to be referenced soon.

    example:

    for(int i = 0; i > arr.length; i++){
        int val = arr[i];
    }


The variable **i** will be kept in memory do to temporal locality. since it will be be accessed each iteration in the loop. 

when arr[i] is read the values from arr[i] to arr[n] will be loaded in to memory since the compiler will expect these values to be read next. 

## Question 1.3

When does a page fault occur?

Answer:

    *page fault* An event that occurs when an accessed page is not present in main memory.

## Question 1.4

What is the use of the Translation Look-aside Buffer (TLB)?

Answer:

    *translation-lookaside buffer* (TLB) A cache that keeps track of recently used address mappings to try to avoid an access to the page table.

## Question 1.5 ??

In this “roofline" diagram for two Opteron processors X2 and X4, explain what happens for X2 and X4 when running a kernel with arithmetic intensity 2.0.

What modifications can be done to improve the performance of X4?

![Alt text](img/Screenshot%202022-11-24%20at%2020.14.32.png)

Answer:

Arithmetic intensity = er på x-aksen.

![Alt text](img/rooflinemodel.png)

When looking at x value = 2; It can be seen that the processor **x2** is computationally bound and **x4** is bound by the memory bandwidth.

We can increase the memory bandwidth of X4 to improve the performance.

# Problem 2


## Question 2.1

Are there structural hazards in the 5-stage RISC-V pipeline? If so, what can you do to mitigate the issue without introducing stalling?

Answer:

    In the case where only a single memory is present there will be a *structural hazards* in a 5-stage pipeline when the *fetch* (F) and *data access* (M) is lining up. 

## Question 2.2 ??

Write RISC-V assembly code for the following fragment of C code. State if you write assembly code for the 32-bit or the 64-bit version of RISC-V (Venus and your simulator is a 32-bit RISC-V). Assume the variables are according to the RISC version either 32-bit or 64-bit long (same as the register width).

A[12] = h + A[8];

h is in x21, base address of A is in x22. The base address is where the array starts in memory.

Answer:

    32-bit version is used 

    ----------------------

    addi t0, x0, 8    // load 8
    slli t1, t0, 3    // t0 = 8*8 byte offset (place 8 times stride of 8-bit)
    add t2, t2, t1    // t2 = &A[i] (calculate address)

    ld t3, 0(t2)      // load value from A[8]
    add t4, x21, t3   // h + A[8]

    addi t5, x0, 12
    slli t6, t5, 3    // t3 = 12*8 byte offset (place 8 times stride of 8-bit)
    add t7, t7, t6    // t2 = &A[i] (calculate address)
    sd t7, 0(t4)      // A[12] = h + A[8];

## Question 2.3 ??

Consider following loop:

    LOOP:
        lw x10, 0(x13)
        lw x11, 8(x13)
        add x12, x10, x11
        subi x13, x13, 16
        bnez x12, LOOP

Assume branches are always correctly predicted (i.e., no stalls due to branches), that there are no delay slots, and that the pipeline has full forwarding support.

Show a pipeline execution diagram for the first two iterations. Mark with a circle (e.g., (E)) the pipeline stages that do not perform useful work. Branches are resolved in EX stage.

How often while the pipeline is full do we have a cycle in which all five pipeline stages are doing useful work? (Begin with the cycle when the *subi* is in the IF stage. End with the cycle when the *bnez* is in the IF stage.)


# Problem 3

The IEEE standard 754 for floating-point includes the specifications for


![Alt text](img/Screenshot%202022-11-26%20at%2017.43.00.png)

represented in floating-point, answer the following questions:


## Question 3.1

    sign = 1 bit
    Precision/faction = 24bit / 11bit
    exponent = 8 bit / 5 bit 

    Note:
    exponent is unsigned number therefore the bias is needed to represent positive an negative numbers. 

#### convert to binary

1000_dec = 11 1110 1000
-993_dec = -11 1110 0001

#### significand (matissa)

To normalize the number the "point (.)" 
is moved from the end and nine (9) places to the left in this case. for at gøre dette på andre tal så skal (.) flyttes til venstre intil der kun er et 1-tal til venstre for (.).

![Alt text](img/Screenshot%202022-11-30%20at%2016.55.06.png)

X = 11 1110 1000. => 1.1 1110 1000 * 2^9
Y = -11 1110 0001. => -1.1 1110 0001 * 2^9

#### Exponent

    To obtain the binary32 representation from (1) we have to add the bias to the exponent and omit the integer bit.

9 kommer fra forige step. bias kommer fra opgaven. normalt 127 for 32bit FP. 

(32 bit) = 9 + 127 = 136 => (10001000)_2
(16 bit) = 9 + 15 = 24 => (11000)_2

Sign er fundet blot ved at se på the orginale tal om the er plus-tal (0) eller et minus-tal (1).


    |      | sign | exp.     | significand +   0's til størrelsen passer |
    |------|------|----------|-------------------------------------------|
    | X_32 | 0    | 10001000 | 11 1110 1000 … 000                        |
    | Y_32 | 1    | 10001000 | 11 1110 0001 … 000                        |
    | X_16 | 0    | 11000    | 11 1110 1000                              |
    | Y_16 | 1    | 11000    | 11 1110 0001                              |


## Question 3.2

Compute the addition of X and Y for **binary32** by using the floating-point addition algorithm. Write the **binary32** representation of the result.

### 1 Significand alignment

Significand alignment is done since they have the same exponent. If they do not have the same you take the lesser exponent (den med det mindste tal) macher det andet tal med det tal. (rykker commaet.)

![Alt text](img/Screenshot%202022-11-30%20at%2019.08.06.png)

### Effective operation is subtraction

1.1111 0100 0000 … 000   
1.1111 0000 1000 … 000  
----------------------  
0.0000 0011 1000 … 000

### Normalization

Normalization is needed. Sum is shifted 7 position to the left and the exponent
is decremented by 7. The sign is positive. No rounding needed.

(10001000)_2 = 136 
new exponent = 136 - 7 = 129 = (10000001)_2

0.0000 0011 1000 … 000 => 00000 001.1 1000 … 000 ??

!!!Forstår ikke hvorfor significand ikke passer her
00000 001.1 1000 … 000 == 


    | sign | exp.     | significand + 0's til størrelsen passer |
    |------|----------|-----------------------------------------|
    | 0    | 10000001 | 11 0000 0000 … 000                      |

## Question 3.3 ?? 

Compute the product of X and Y for **binary16** by using the floating-point multi- plication algorithm. Write the **binary16** representation of the result.


# Problem 4

## Question 4.1 ?? 

Media applications that play audio or video files are part of a class of workloads called “streaming” workloads; i.e., they bring large amounts of data but do not reuse much of it. Consider a video streaming workload that accesses 1024 KB working set sequentially with the following address stream (byte addresses): 

    0, 2, 4, 6, 8, 10, 12, 14, 16, ...

Assume a 32 KB direct-mapped cache with a 32-byte block. What is the miss rate for the address stream above? How is this miss rate sensitive to the size of the cache or the working set? How would you categorize the misses (compulsory, conflict, or capacity) this workload is experiencing?

Answer:

byte max adresss length = 5 bit

    Assuming the addresses given as byte addresses, each group of 16 accesses will map to the same 32-byte block so the cache will have a miss rate of 1/16. The miss rate is not sensitive to the size of the cache or the size of the working set. It is, how- ever, sensitive to the access pattern and block size. All misses are compulsory misses.


## Question 4.2 ??

